COMPARISON DIFF
============================================================

Original size: 89146 bytes
Chloro size:   88867 bytes
Rustfmt size:  89146 bytes

âœ— Outputs DIFFER

--- DIFF (- rustfmt, + chloro) ---
 //! Module responsible for analyzing the code surrounding the cursor for completion.
+
 use std::iter;
 
 use hir::{ExpandResult, InFile, Semantics, Type, TypeInfo, Variant};
     pub(super) original_offset: TextSize,
 }
 
-pub(super) fn expand_and_analyze<'db>(
-    sema: &Semantics<'db, RootDatabase>,
-    original_file: InFile<SyntaxNode>,
-    speculative_file: SyntaxNode,
-    offset: TextSize,
-    original_token: &SyntaxToken,
-) -> Option<AnalysisResult<'db>> {
+pub(super) fn expand_and_analyze<'db>(sema: &Semantics<'db, RootDatabase>, original_file: InFile<SyntaxNode>, speculative_file: SyntaxNode, offset: TextSize, original_token: &SyntaxToken) -> Option<AnalysisResult<'db>> {
     // as we insert after the offset, right biased will *always* pick the identifier no matter
     // if there is an ident already typed or not
     let fake_ident_token = speculative_file.token_at_offset(offset).right_biased()?;
         fake_ident_token,
         derive_ctx: None,
     });
-
     // add the relative offset back, so that left_biased finds the proper token
     let original_offset = expansion.original_offset + relative_offset;
     let token = expansion.original_file.token_at_offset(original_offset).left_biased()?;
-
     hir::attach_db(sema.db, || analyze(sema, expansion, original_token, &token)).map(
         |(analysis, expected, qualifier_ctx)| AnalysisResult {
             analysis,
 /// that we check, we subtract `COMPLETION_MARKER.len()`. This may not be accurate because proc macros
 /// can insert the text of the completion marker in other places while removing the span, but this is
 /// the best we can do.
-fn expand_maybe_stop(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: InFile<SyntaxNode>,
-    speculative_file: SyntaxNode,
-    original_offset: TextSize,
-    fake_ident_token: SyntaxToken,
-    relative_offset: TextSize,
-) -> Option<ExpansionResult> {
+fn expand_maybe_stop(sema: &Semantics<'_, RootDatabase>, original_file: InFile<SyntaxNode>, speculative_file: SyntaxNode, original_offset: TextSize, fake_ident_token: SyntaxToken, relative_offset: TextSize) -> Option<ExpansionResult> {
     if let result @ Some(_) = expand(
         sema,
         original_file.clone(),
     ) {
         return result;
     }
-
     // We can't check whether the fake expansion is inside macro call, because that requires semantic info.
     // But hopefully checking just the real one should be enough.
     if token_at_offset_ignore_whitespace(&original_file.value, original_offset + relative_offset)
     }
 }
 
-fn expand(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: InFile<SyntaxNode>,
-    speculative_file: SyntaxNode,
-    original_offset: TextSize,
-    fake_ident_token: SyntaxToken,
-    relative_offset: TextSize,
-) -> Option<ExpansionResult> {
+fn expand(sema: &Semantics<'_, RootDatabase>, original_file: InFile<SyntaxNode>, speculative_file: SyntaxNode, original_offset: TextSize, fake_ident_token: SyntaxToken, relative_offset: TextSize) -> Option<ExpansionResult> {
     let _p = tracing::info_span!("CompletionContext::expand").entered();
-
     let parent_item =
         |item: &ast::Item| item.syntax().ancestors().skip(1).find_map(ast::Item::cast);
     let original_node = token_at_offset_ignore_whitespace(&original_file.value, original_offset)
         ),
         |(a, b)| parent_item(a).zip(parent_item(b)),
     );
-
     // first try to expand attributes as these are always the outermost macro calls
     'ancestors: for (actual_item, item_with_fake_ident) in ancestor_items {
         match (
             _ => break 'ancestors,
         }
     }
-
     // No attributes have been expanded, so look for macro_call! token trees or derive token trees
     let orig_tt = ancestors_at_offset(&original_file.value, original_offset)
         .map_while(Either::<ast::TokenTree, ast::Meta>::cast)
     let spec_tt = ancestors_at_offset(&speculative_file, fake_ident_token.text_range().start())
         .map_while(Either::<ast::TokenTree, ast::Meta>::cast)
         .last()?;
-
     let (tts, attrs) = match (orig_tt, spec_tt) {
         (Either::Left(orig_tt), Either::Left(spec_tt)) => {
             let attrs = orig_tt
         }
         _ => return None,
     };
-
     // Expand pseudo-derive expansion aka `derive(Debug$0)`
     if let Some((orig_attr, spec_attr)) = attrs {
         if let (Some(actual_expansion), Some((fake_expansion, fake_mapped_tokens))) = (
         // at this point we won't have any more successful expansions, so stop
         return None;
     }
-
     // Expand fn-like macro calls
     let (orig_tt, spec_tt) = tts?;
     let (actual_macro_call, macro_call_with_fake_ident) = (
     );
     let mac_call_path0 = actual_macro_call.path().as_ref().map(|s| s.syntax().text());
     let mac_call_path1 = macro_call_with_fake_ident.path().as_ref().map(|s| s.syntax().text());
-
     // inconsistent state, stop expanding
     if mac_call_path0 != mac_call_path1 {
         return None;
     }
     let speculative_args = macro_call_with_fake_ident.token_tree()?;
-
     match (
         sema.expand_macro_call(&actual_macro_call),
         sema.speculative_expand_macro_call(&actual_macro_call, &speculative_args, fake_ident_token),
 
 /// Fill the completion context, this is what does semantic reasoning about the surrounding context
 /// of the completion location.
-fn analyze<'db>(
-    sema: &Semantics<'db, RootDatabase>,
-    expansion_result: ExpansionResult,
-    original_token: &SyntaxToken,
-    self_token: &SyntaxToken,
-) -> Option<(CompletionAnalysis<'db>, (Option<Type<'db>>, Option<ast::NameOrNameRef>), QualifierCtx)>
-{
+fn analyze<'db>(sema: &Semantics<'db, RootDatabase>, expansion_result: ExpansionResult, original_token: &SyntaxToken, self_token: &SyntaxToken) -> Option<(CompletionAnalysis<'db>, (Option<Type<'db>>, Option<ast::NameOrNameRef>), QualifierCtx)> {
     let _p = tracing::info_span!("CompletionContext::analyze").entered();
     let ExpansionResult {
         original_file,
         fake_ident_token,
         derive_ctx,
     } = expansion_result;
-
     if original_token.kind() != self_token.kind()
         // FIXME: This check can be removed once we use speculative database forking for completions
         && !(original_token.kind().is_punct() || original_token.kind().is_trivia())
     {
         return None;
     }
-
     // Overwrite the path kind for derives
     if let Some((original_file, file_with_fake_ident, offset, origin_attr)) = derive_ctx {
         if let Some(ast::NameLike::NameRef(name_ref)) =
         }
         return None;
     }
-
     let Some(name_like) = find_node_at_offset(&speculative_file, speculative_offset) else {
         let analysis = if let Some(original) = ast::String::cast(original_token.clone()) {
             CompletionAnalysis::String { original, expanded: ast::String::cast(self_token.clone()) }
         };
         return Some((analysis, (None, None), QualifierCtx::default()));
     };
-
     let expected = expected_type_and_name(sema, self_token, &name_like);
     let mut qual_ctx = QualifierCtx::default();
     let analysis = match name_like {
 }
 
 /// Calculate the expected type and name of the cursor position.
-fn expected_type_and_name<'db>(
-    sema: &Semantics<'db, RootDatabase>,
-    self_token: &SyntaxToken,
-    name_like: &ast::NameLike,
-) -> (Option<Type<'db>>, Option<NameOrNameRef>) {
+fn expected_type_and_name<'db>(sema: &Semantics<'db, RootDatabase>, self_token: &SyntaxToken, name_like: &ast::NameLike) -> (Option<Type<'db>>, Option<NameOrNameRef>) {
     let token = prev_special_biased_token_at_trivia(self_token.clone());
     let mut node = match token.parent() {
         Some(it) => it,
         None => return (None, None),
     };
-
     let strip_refs = |mut ty: Type<'db>| match name_like {
         ast::NameLike::NameRef(n) => {
             let p = match n.syntax().parent() {
         }
         _ => ty,
     };
-
     let (ty, name) = loop {
         break match_ast! {
             match node {
     (ty.map(strip_refs), name)
 }
 
-fn classify_lifetime(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: &SyntaxNode,
-    lifetime: ast::Lifetime,
-) -> Option<LifetimeContext> {
+fn classify_lifetime(sema: &Semantics<'_, RootDatabase>, original_file: &SyntaxNode, lifetime: ast::Lifetime) -> Option<LifetimeContext> {
     let parent = lifetime.syntax().parent()?;
     if parent.kind() == SyntaxKind::ERROR {
         return None;
     }
-
     let lifetime =
         find_node_at_offset::<ast::Lifetime>(original_file, lifetime.syntax().text_range().start());
     let kind = match_ast! {
             },
         }
     };
-
     Some(LifetimeContext { kind })
 }
 
-fn classify_name(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: &SyntaxNode,
-    name: ast::Name,
-) -> Option<NameContext> {
+fn classify_name(sema: &Semantics<'_, RootDatabase>, original_file: &SyntaxNode, name: ast::Name) -> Option<NameContext> {
     let parent = name.syntax().parent()?;
     let kind = match_ast! {
         match parent {
     Some(NameContext { name, kind })
 }
 
-fn classify_name_ref<'db>(
-    sema: &Semantics<'db, RootDatabase>,
-    original_file: &SyntaxNode,
-    name_ref: ast::NameRef,
-    original_offset: TextSize,
-    parent: SyntaxNode,
-) -> Option<(NameRefContext<'db>, QualifierCtx)> {
+fn classify_name_ref<'db>(sema: &Semantics<'db, RootDatabase>, original_file: &SyntaxNode, name_ref: ast::NameRef, original_offset: TextSize, parent: SyntaxNode) -> Option<(NameRefContext<'db>, QualifierCtx)> {
     let nameref = find_node_at_offset(original_file, original_offset);
-
     let make_res = |kind| (NameRefContext { nameref: nameref.clone(), kind }, Default::default());
-
     if let Some(record_field) = ast::RecordExprField::for_field_name(&name_ref) {
         let dot_prefix = previous_non_trivia_token(name_ref.syntax().clone())
             .is_some_and(|it| T![.] == it.kind());
         });
         return Some(make_res(kind));
     }
-
     let field_expr_handle = |receiver, node| {
         let receiver = find_opt_node_in_file(original_file, receiver);
         let receiver_is_ambiguous_float_literal = match &receiver {
         });
         Some(make_res(kind))
     };
-
     let segment = match_ast! {
         match parent {
             ast::PathSegment(segment) => segment,
             _ => return None,
         }
     };
-
     let path = segment.parent_path();
     let original_path = find_node_in_file_compensated(sema, original_file, &path);
-
     let mut path_ctx = PathCompletionCtx {
         has_call_parens: false,
         has_macro_bang: false,
         has_type_args: false,
         use_tree_parent: false,
     };
-
     let func_update_record = |syn: &SyntaxNode| {
         if let Some(record_expr) = syn.ancestors().nth(2).and_then(ast::RecordExpr::cast) {
             find_node_in_file_compensated(sema, original_file, &record_expr)
                 .and_then(|expr| expr.index())
                 .is_some_and(|expr| expr.syntax() == it)
     };
-
     // We do not want to generate path completions when we are sandwiched between an item decl signature and its body.
     // ex. trait Foo $0 {}
     // in these cases parser recovery usually kicks in for our inserted identifier, causing it
         }
         None
     };
-
     let generic_arg_location = |arg: ast::GenericArg| {
         let mut override_location = None;
         let location = find_opt_node_in_file_compensated(
             corresponding_param,
         })
     };
-
     let type_location = |node: &SyntaxNode| {
         let parent = node.parent()?;
         let res = match_ast! {
         };
         Some(res)
     };
-
     let make_path_kind_expr = |expr: ast::Expr| {
         let it = expr.syntax();
         let in_block_expr = is_in_block(it);
         let location = type_location(ty.syntax());
         PathKind::Type { location: location.unwrap_or(TypeLocation::Other) }
     };
-
     let kind_item = |it: &SyntaxNode| {
         let parent = it.parent()?;
         let kind = match_ast! {
         };
         Some(kind)
     };
-
     let mut kind_macro_call = |it: ast::MacroCall| {
         path_ctx.has_macro_bang = it.excl_token().is_some();
         let parent = it.syntax().parent()?;
             .unwrap_or_default();
         Some(PathKind::Attr { attr_ctx: AttrCtx { kind, annotated_item_kind, derive_helpers } })
     };
-
     // Infer the path kind
     let parent = path.syntax().parent()?;
     let kind = 'find_kind: {
             }
         }
     };
-
     path_ctx.kind = kind;
     path_ctx.has_type_args = segment.generic_arg_list().is_some();
-
     // calculate the qualifier context
     if let Some((qualifier, use_tree_parent)) = path_or_use_tree_qualifier(&path) {
         path_ctx.use_tree_parent = use_tree_parent;
     {
         path_ctx.qualified = Qualified::Absolute;
     }
-
     let mut qualifier_ctx = QualifierCtx::default();
     if path_ctx.is_trivial_path() {
         // fetch the full expression that may have qualifiers attached to it
         .all(|whitespace| !whitespace.text().contains('\n'))
 }
 
-fn pattern_context_for(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: &SyntaxNode,
-    pat: ast::Pat,
-) -> PatternContext {
+fn pattern_context_for(sema: &Semantics<'_, RootDatabase>, original_file: &SyntaxNode, pat: ast::Pat) -> PatternContext {
     let mut param_ctx = None;
-
     let mut missing_variants = vec![];
-
     let (refutability, has_type_ascription) =
     pat
         .syntax()
         ast::Pat::IdentPat(it) => (it.ref_token(), it.mut_token()),
         _ => (None, None),
     };
-
     // Only suggest name in let-stmt or fn param
     let should_suggest_name = matches!(
             &pat,
                     ast::LetStmt::can_cast(kind) || ast::Param::can_cast(kind)
                 })
     );
-
     PatternContext {
         refutability,
         param_ctx,
     }
 }
 
-fn fetch_immediate_impl_or_trait(
-    sema: &Semantics<'_, RootDatabase>,
-    original_file: &SyntaxNode,
-    node: &SyntaxNode,
-) -> Option<Either<ast::Impl, ast::Trait>> {
+fn fetch_immediate_impl_or_trait(sema: &Semantics<'_, RootDatabase>, original_file: &SyntaxNode, node: &SyntaxNode) -> Option<Either<ast::Impl, ast::Trait>> {
     let mut ancestors = ancestors_in_file_compensated(sema, original_file, node)?
         .filter_map(ast::Item::cast)
         .filter(|it| !matches!(it, ast::Item::MacroCall(_)));
-
     match ancestors.next()? {
         ast::Item::Const(_) | ast::Item::Fn(_) | ast::Item::TypeAlias(_) => (),
         ast::Item::Impl(it) => return Some(Either::Left(it)),
 /// Attempts to find `node` inside `syntax` via `node`'s text range while compensating
 /// for the offset introduced by the fake ident.
 /// This is wrong if `node` comes before the insertion point! Use `find_node_in_file` instead.
-fn find_node_in_file_compensated<N: AstNode>(
-    sema: &Semantics<'_, RootDatabase>,
-    in_file: &SyntaxNode,
-    node: &N,
-) -> Option<N> {
+fn find_node_in_file_compensated<N: AstNode>(sema: &Semantics<'_, RootDatabase>, in_file: &SyntaxNode, node: &N) -> Option<N> {
     ancestors_in_file_compensated(sema, in_file, node.syntax())?.find_map(N::cast)
 }
 
-fn ancestors_in_file_compensated<'sema>(
-    sema: &'sema Semantics<'_, RootDatabase>,
-    in_file: &SyntaxNode,
-    node: &SyntaxNode,
-) -> Option<impl Iterator<Item = SyntaxNode> + 'sema> {
+fn ancestors_in_file_compensated<'sema>(sema: &'sema Semantics<'_, RootDatabase>, in_file: &SyntaxNode, node: &SyntaxNode) -> Option<impl Iterator<Item = SyntaxNode> + 'sema> {
     let syntax_range = in_file.text_range();
     let range = node.text_range();
     let end = range.end().checked_sub(TextSize::try_from(COMPLETION_MARKER.len()).ok()?)?;
 /// Attempts to find `node` inside `syntax` via `node`'s text range while compensating
 /// for the offset introduced by the fake ident..
 /// This is wrong if `node` comes before the insertion point! Use `find_node_in_file` instead.
-fn find_opt_node_in_file_compensated<N: AstNode>(
-    sema: &Semantics<'_, RootDatabase>,
-    syntax: &SyntaxNode,
-    node: Option<N>,
-) -> Option<N> {
+fn find_opt_node_in_file_compensated<N: AstNode>(sema: &Semantics<'_, RootDatabase>, syntax: &SyntaxNode, node: Option<N>) -> Option<N> {
     find_node_in_file_compensated(sema, syntax, &node?)
 }
 